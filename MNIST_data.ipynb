{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 01 - Classify the 10 digits using MNIST data\n",
        "\n",
        "Implemented the convolutional neural network using purely numpy to classify the digits.\n",
        "\n",
        "## Data Description\n",
        "[MNIST](https://en.wikipedia.org/wiki/MNIST_database)"
      ],
      "metadata": {
        "id": "T9YlttlK6gQg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHmJkmgkvOFq"
      },
      "outputs": [],
      "source": [
        "# load the mnist data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSAPH5DgvV_F",
        "outputId": "11b71b46-e032-4340-97ab-d858951f43d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "mnist_28 = fetch_openml('mnist_784')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mf2eo_YvXga"
      },
      "outputs": [],
      "source": [
        "mnist_28_img= mnist_28.data.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zhdUa0pvY59",
        "outputId": "e5aaa7a1-cf7f-4c6d-9ab2-3a99700c8c95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "mnist_28_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm7m9wRBva23"
      },
      "outputs": [],
      "source": [
        "#display one image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "OChedIeevcaM",
        "outputId": "582387f3-1008-4ae7-8f07-3cd7641eecae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a0928c933d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow((mnist_28_img[1].reshape(28,28)), cmap=plt.cm.gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD1Ypw16Vjgt",
        "outputId": "c7184ab7-97d5-4071-f01d-cb11cd8b1e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: 49000\n",
            "Size of testing set: 21000\n"
          ]
        }
      ],
      "source": [
        "mnist = fetch_openml('mnist_784')\n",
        "X = mnist.data.to_numpy()\n",
        "y = mnist.target.to_numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print('Size of training set:', len(X_train))\n",
        "print('Size of testing set:', len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cel_1OaVp9U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Convert the input data to a NumPy array\n",
        "X = mnist_28.data.astype(\"float32\")\n",
        "\n",
        "# Convert the target labels to a NumPy array\n",
        "y = mnist_28.target.astype(\"int\")\n",
        "\n",
        "# Create a LabelBinarizer object\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "# One-hot encode the target labels\n",
        "y_one_hot = lb.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_one_hot, test_size=0.3, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVESIex6VqKg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, X_train, X_test):\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "\n",
        "    def reshape_data(self):\n",
        "        \"\"\"Reshapes the NumPy arrays to the expected format.\"\"\"\n",
        "        self.X_train = self.X_train.values.reshape(-1, 28, 28, 1)\n",
        "        self.X_test = self.X_test.values.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    def normalize_data(self):\n",
        "        \"\"\"Normalizes the pixel values between 0 and 1.\"\"\"\n",
        "        self.X_train = self.X_train.astype('float32') / 255\n",
        "        self.X_test = self.X_test.astype('float32') / 255\n",
        "\n",
        "    def get_preprocessed_data(self):\n",
        "        \"\"\"Returns the preprocessed training and testing data.\"\"\"\n",
        "        return self.X_train, self.X_test\n",
        "\n",
        "# Assuming X_train and X_test are DataFrames, and you have loaded your data\n",
        "# Create a data preprocessor object\n",
        "data_preprocessor = DataPreprocessor(X_train, X_test)\n",
        "\n",
        "# Preprocess the data\n",
        "data_preprocessor.reshape_data()\n",
        "data_preprocessor.normalize_data()\n",
        "\n",
        "# Get the preprocessed training and testing data\n",
        "X_train_preprocessed, X_test_preprocessed = data_preprocessor.get_preprocessed_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXHHnbSIexlR",
        "outputId": "c7d0df7f-5191-497e-f687-f9dc013ebdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (49000, 784)\n",
            "Testing data shape: (21000, 784)\n"
          ]
        }
      ],
      "source": [
        "def test_train_shape(X_train, X_test):\n",
        "    print(\"Training data shape:\", X_train.shape)\n",
        "    print(\"Testing data shape:\", X_test.shape)\n",
        "\n",
        "# Call the check_shape function\n",
        "test_train_shape(X_train, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a neural network using the ReLU activation function"
      ],
      "metadata": {
        "id": "KFypUU-VZP8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03fH22idzQA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define relu_derivative\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, num_layers, num_neurons_per_layer, learning_rate):\n",
        "        self.num_layers = num_layers\n",
        "        self.num_neurons_per_layer = num_neurons_per_layer\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize the weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(num_layers - 1):\n",
        "            self.weights.append(np.random.randn(num_neurons_per_layer[i], num_neurons_per_layer[i + 1]))\n",
        "            self.biases.append(np.zeros(num_neurons_per_layer[i + 1]))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        layer_input = X\n",
        "        layer_outputs = [layer_input]\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            layer_input = relu(np.dot(layer_input, self.weights[i]) + self.biases[i])\n",
        "            layer_outputs.append(layer_input)\n",
        "\n",
        "        return layer_outputs\n",
        "\n",
        "    def backward_propagation(self, X, y, layer_outputs):\n",
        "        num_layers = len(layer_outputs) - 1\n",
        "        output_error = y - layer_outputs[num_layers]\n",
        "        delta = output_error * relu_derivative(layer_outputs[num_layers])\n",
        "\n",
        "        for i in range(num_layers, 0, -1):\n",
        "            self.weights[i - 1] += self.learning_rate * np.dot(layer_outputs[i - 1].T, delta)\n",
        "            self.biases[i - 1] += self.learning_rate * np.sum(delta, axis=0)\n",
        "            delta = np.dot(delta, self.weights[i - 1].T) * relu_derivative(layer_outputs[i - 1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        layer_outputs = self.forward_propagation(X)\n",
        "        prediction = layer_outputs[-1]\n",
        "        return prediction\n",
        "\n",
        "    def calculate_accuracy(self, predictions, true_labels):\n",
        "        num_samples = true_labels.shape[0]\n",
        "        res_predict = np.sum(np.argmax(predictions, axis=1) == np.argmax(true_labels, axis=1))\n",
        "        model_accuracy = res_predict / num_samples\n",
        "        return model_accuracy\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            layer_outputs = self.forward_propagation(X)\n",
        "            self.backward_propagation(X, y, layer_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05-2SlnTeZHe",
        "outputId": "f9b04fd3-c1bd-487b-86cd-7abd98429ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.098\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "no_layers = 3\n",
        "no_neurons = [784, 128, 10]\n",
        "model_learning_rate = 0.01\n",
        "\n",
        "\n",
        "model = NeuralNetwork(no_layers, no_neurons, model_learning_rate)\n",
        "model.train(X_train, y_train, epochs)\n",
        "prediction = model.predict(X_test)\n",
        "model_accuracy = model.calculate_accuracy(prediction, y_test)\n",
        "print(\"Accuracy:\", model_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "inter"
      ],
      "metadata": {
        "id": "hnW1-yzqzukw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30R4HMLQgSoU"
      },
      "source": [
        "##Neural Network with Sigmoid Activation and Regularization for Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p4GDIMHfObx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ6fSZDOgYDp"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(no_layers, no_neurons):\n",
        "    weights = []\n",
        "    for i in range(no_layers - 1):\n",
        "        weights.append(np.random.randn(no_neurons[i], no_neurons[i + 1]) * np.sqrt(2 / no_neurons[i]))\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V917QZPJgZ_W"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, no_layers, no_neurons, model_learning_rate, activation_function, regularization_lambda):\n",
        "        self.no_layers = no_layers\n",
        "        self.no_neurons = no_neurons\n",
        "        self.model_learning_rate = model_learning_rate\n",
        "        self.activation_function = activation_function\n",
        "        self.regularization_lambda = regularization_lambda\n",
        "        self.weights = initialize_weights(no_layers=no_layers, no_neurons=no_neurons)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        layer_input = X\n",
        "        layer_outputs = [layer_input]\n",
        "        for i in range(self.no_layers - 1):\n",
        "            layer_input = self.activation_function(np.dot(layer_input, self.weights[i]))\n",
        "            layer_outputs.append(layer_input)\n",
        "        return layer_outputs\n",
        "\n",
        "    def backward_propagation(self, X, y, layer_outputs):\n",
        "        no_layers = len(layer_outputs) - 1\n",
        "        output_error = y - layer_outputs[no_layers]\n",
        "        delta = output_error * sigmoid_derivative(layer_outputs[no_layers])\n",
        "        for i in range(no_layers, 0, -1):\n",
        "            self.weights[i - 1] += self.model_learning_rate * (np.dot(layer_outputs[i - 1].T, delta) - self.regularization_lambda * self.weights[i - 1])\n",
        "            delta = np.dot(delta, self.weights[i - 1].T) * sigmoid_derivative(layer_outputs[i - 1])\n",
        "\n",
        "    def predict(self, X):\n",
        "        layer_outputs = self.forward_propagation(X)\n",
        "        prediction = layer_outputs[-1]\n",
        "        return prediction\n",
        "\n",
        "    def calculate_accuracy(self, predictions, true_labels):\n",
        "        num_samples = true_labels.shape[0]\n",
        "        res_predict = np.sum(np.argmax(predictions, axis=1) == np.argmax(true_labels, axis=1))\n",
        "        model_accuracy = res_predict / num_samples\n",
        "        return model_accuracy\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            layer_outputs = self.forward_propagation(X)\n",
        "            self.backward_propagation(X, y, layer_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuHkp9lagcId",
        "outputId": "f3946fbd-0da6-4d0d-b283-6fbffc284ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-58ac12a57fa0>:4: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.10157142857142858\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "no_layers = 3\n",
        "no_neurons = [784, 128, 10]\n",
        "model_learning_rate = 0.01\n",
        "regularization_lambda = 0.01\n",
        "activation_function = sigmoid\n",
        "\n",
        "model = NeuralNetwork(no_layers, no_neurons, model_learning_rate, activation_function, regularization_lambda)\n",
        "model.train(X_train, y_train, epochs)\n",
        "\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "model_accuracy = model.calculate_accuracy(prediction, y_test)\n",
        "print(\"Accuracy:\", model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "inter"
      ],
      "metadata": {
        "id": "Cyynfxczzr65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Regression with Regularization for Image Classification."
      ],
      "metadata": {
        "id": "XAeSFqX74QdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data (replace with your actual training data)\n",
        "num_samples = 49000\n",
        "\n",
        "image_size = 28 * 28\n",
        "num_classes = 10\n",
        "\n",
        "train_images = np.random.rand(num_samples, image_size)\n",
        "train_labels = np.random.randint(0, num_classes, num_samples)\n",
        "# Example data (replace with your actual test data)\n",
        "num_test_samples = 21000  # Replace with the number of test samples you have\n",
        "test_images = np.random.rand(num_test_samples, image_size)\n",
        "test_labels = np.random.randint(0, num_classes, num_test_samples)\n"
      ],
      "metadata": {
        "id": "UW3DsKSuD7tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVFdOrdREfEt",
        "outputId": "01d7d55d-1789-405b-d5ee-1f6721121567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data (replace with your actual training and test data)\n",
        "num_train_samples = 49000\n",
        "num_test_samples = 21000\n",
        "image_size = 28 * 28\n",
        "num_classes = 10\n",
        "\n",
        "train_images = np.random.rand(num_train_samples, image_size)\n",
        "train_labels = np.random.randint(0, num_classes, num_train_samples)\n",
        "test_images = np.random.rand(num_test_samples, image_size)\n",
        "test_labels = np.random.randint(0, num_classes, num_test_samples)\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self, input_node, softmax_node):\n",
        "        self.weight = np.random.randn(input_node, softmax_node) / np.sqrt(input_node)\n",
        "        self.bias = np.zeros(softmax_node)\n",
        "        self.modified_input = None\n",
        "\n",
        "    def forward_prop(self, image, label=None):\n",
        "        image_normalized = (image / 255) - 0.5\n",
        "        logits = np.dot(image_normalized, self.weight) + self.bias\n",
        "        max_logit = np.max(logits)\n",
        "        exp_logits = np.exp(logits - max_logit)\n",
        "        out_p = exp_logits / np.sum(exp_logits)\n",
        "\n",
        "        if label is not None:\n",
        "            cross_ent_loss = -np.log(out_p[label] + 1e-8)  # Add a small constant to avoid log(0)\n",
        "            return out_p, cross_ent_loss\n",
        "        else:\n",
        "            return out_p\n",
        "\n",
        "    def back_prop(self, dL_dout, learning_rate):\n",
        "        dL_dw = np.outer(self.modified_input, dL_dout)\n",
        "        dL_db = dL_dout\n",
        "        self.weight -= learning_rate * dL_dw\n",
        "        self.bias -= learning_rate * dL_db\n",
        "\n",
        "    def train(self, image, label, learning_rate=0.001):\n",
        "        self.modified_input = image.flatten()\n",
        "        out_p, loss = self.forward_prop(image, label)\n",
        "        gradient = np.zeros(10)\n",
        "        gradient[label] = 1\n",
        "        self.back_prop(out_p - gradient, learning_rate)\n",
        "        accuracy_eval = 1 if np.argmax(out_p) == label else 0\n",
        "\n",
        "        return loss, accuracy_eval\n",
        "\n",
        "softmax = Softmax(28 * 28, 10)\n",
        "\n",
        "num_epochs = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch', epoch + 1)\n",
        "\n",
        "    # Shuffle the training data\n",
        "    shuffle_indices = np.random.permutation(num_train_samples)\n",
        "    train_images = train_images[shuffle_indices]\n",
        "    train_labels = train_labels[shuffle_indices]\n",
        "\n",
        "    loss = 0\n",
        "    num_correct = 0\n",
        "\n",
        "    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "        if i % 1000 == 0:\n",
        "            avg_loss = loss / 1000\n",
        "            avg_accuracy = num_correct / 10\n",
        "            print(f'{i // 1000 + 1} steps out of 49 steps: Average_loss: {avg_loss:.5f} and Accuracy: {avg_accuracy:.2f}')\n",
        "            loss = 0\n",
        "            num_correct = 0\n",
        "\n",
        "        l1, accu = softmax.train(im, label, learning_rate)\n",
        "        loss += l1\n",
        "        num_correct += accu\n",
        "\n",
        "print('Testing')\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for im, label in zip(test_images, test_labels):\n",
        "    out_p = softmax.forward_prop(im)\n",
        "    loss += out_p[label]\n",
        "    num_correct += 1 if np.argmax(out_p) == label else 0\n",
        "\n",
        "num_tests = len(test_images)\n",
        "print(f'Test loss: {loss / num_tests:.5f}')\n",
        "print(f'Test Accuracy: {num_correct * 1000 / num_tests:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_brBZhzwVpFp",
        "outputId": "cde82f8c-7d89-4eaf-891a-380e7d74e556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "1 steps out of 49 steps: Average_loss: 0.00000 and Accuracy: 0.00\n",
            "2 steps out of 49 steps: Average_loss: 14.75268 and Accuracy: 11.00\n",
            "3 steps out of 49 steps: Average_loss: 16.57861 and Accuracy: 10.00\n",
            "4 steps out of 49 steps: Average_loss: 16.35756 and Accuracy: 11.20\n",
            "5 steps out of 49 steps: Average_loss: 16.32072 and Accuracy: 11.40\n",
            "6 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "7 steps out of 49 steps: Average_loss: 16.81808 and Accuracy: 8.70\n",
            "8 steps out of 49 steps: Average_loss: 16.68914 and Accuracy: 9.40\n",
            "9 steps out of 49 steps: Average_loss: 16.41283 and Accuracy: 10.90\n",
            "10 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "11 steps out of 49 steps: Average_loss: 16.63387 and Accuracy: 9.70\n",
            "12 steps out of 49 steps: Average_loss: 16.37599 and Accuracy: 11.10\n",
            "13 steps out of 49 steps: Average_loss: 16.28388 and Accuracy: 11.60\n",
            "14 steps out of 49 steps: Average_loss: 16.63387 and Accuracy: 9.70\n",
            "15 steps out of 49 steps: Average_loss: 16.85492 and Accuracy: 8.50\n",
            "16 steps out of 49 steps: Average_loss: 16.48651 and Accuracy: 10.50\n",
            "17 steps out of 49 steps: Average_loss: 16.57861 and Accuracy: 10.00\n",
            "18 steps out of 49 steps: Average_loss: 16.44967 and Accuracy: 10.70\n",
            "19 steps out of 49 steps: Average_loss: 16.54177 and Accuracy: 10.20\n",
            "20 steps out of 49 steps: Average_loss: 17.07597 and Accuracy: 7.30\n",
            "21 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "22 steps out of 49 steps: Average_loss: 16.28388 and Accuracy: 11.60\n",
            "23 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "24 steps out of 49 steps: Average_loss: 16.76282 and Accuracy: 9.00\n",
            "25 steps out of 49 steps: Average_loss: 16.41283 and Accuracy: 10.90\n",
            "26 steps out of 49 steps: Average_loss: 16.65230 and Accuracy: 9.60\n",
            "27 steps out of 49 steps: Average_loss: 16.65230 and Accuracy: 9.60\n",
            "28 steps out of 49 steps: Average_loss: 16.56019 and Accuracy: 10.10\n",
            "29 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "30 steps out of 49 steps: Average_loss: 16.67072 and Accuracy: 9.50\n",
            "31 steps out of 49 steps: Average_loss: 16.46809 and Accuracy: 10.60\n",
            "32 steps out of 49 steps: Average_loss: 16.48651 and Accuracy: 10.50\n",
            "33 steps out of 49 steps: Average_loss: 16.65230 and Accuracy: 9.60\n",
            "34 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "35 steps out of 49 steps: Average_loss: 16.52335 and Accuracy: 10.30\n",
            "36 steps out of 49 steps: Average_loss: 16.91018 and Accuracy: 8.20\n",
            "37 steps out of 49 steps: Average_loss: 16.76282 and Accuracy: 9.00\n",
            "38 steps out of 49 steps: Average_loss: 16.67072 and Accuracy: 9.50\n",
            "39 steps out of 49 steps: Average_loss: 16.61545 and Accuracy: 9.80\n",
            "40 steps out of 49 steps: Average_loss: 16.70756 and Accuracy: 9.30\n",
            "41 steps out of 49 steps: Average_loss: 16.57861 and Accuracy: 10.00\n",
            "42 steps out of 49 steps: Average_loss: 16.43125 and Accuracy: 10.80\n",
            "43 steps out of 49 steps: Average_loss: 16.63387 and Accuracy: 9.70\n",
            "44 steps out of 49 steps: Average_loss: 16.48651 and Accuracy: 10.50\n",
            "45 steps out of 49 steps: Average_loss: 16.63387 and Accuracy: 9.70\n",
            "46 steps out of 49 steps: Average_loss: 16.39441 and Accuracy: 11.00\n",
            "47 steps out of 49 steps: Average_loss: 16.33914 and Accuracy: 11.30\n",
            "48 steps out of 49 steps: Average_loss: 16.70756 and Accuracy: 9.30\n",
            "49 steps out of 49 steps: Average_loss: 16.61545 and Accuracy: 9.80\n",
            "Epoch 2\n",
            "1 steps out of 49 steps: Average_loss: 0.00000 and Accuracy: 0.00\n",
            "2 steps out of 49 steps: Average_loss: 16.48651 and Accuracy: 10.50\n",
            "3 steps out of 49 steps: Average_loss: 16.52335 and Accuracy: 10.30\n",
            "4 steps out of 49 steps: Average_loss: 16.67072 and Accuracy: 9.50\n",
            "5 steps out of 49 steps: Average_loss: 16.61545 and Accuracy: 9.80\n",
            "6 steps out of 49 steps: Average_loss: 16.72598 and Accuracy: 9.20\n",
            "7 steps out of 49 steps: Average_loss: 16.43125 and Accuracy: 10.80\n",
            "8 steps out of 49 steps: Average_loss: 16.65230 and Accuracy: 9.60\n",
            "9 steps out of 49 steps: Average_loss: 16.37599 and Accuracy: 11.10\n",
            "10 steps out of 49 steps: Average_loss: 16.63387 and Accuracy: 9.70\n",
            "11 steps out of 49 steps: Average_loss: 16.41283 and Accuracy: 10.90\n",
            "12 steps out of 49 steps: Average_loss: 16.50493 and Accuracy: 10.40\n",
            "13 steps out of 49 steps: Average_loss: 16.56019 and Accuracy: 10.10\n",
            "14 steps out of 49 steps: Average_loss: 16.41283 and Accuracy: 10.90\n",
            "15 steps out of 49 steps: Average_loss: 16.65230 and Accuracy: 9.60\n",
            "16 steps out of 49 steps: Average_loss: 16.78124 and Accuracy: 8.90\n",
            "17 steps out of 49 steps: Average_loss: 16.43125 and Accuracy: 10.80\n",
            "18 steps out of 49 steps: Average_loss: 16.59703 and Accuracy: 9.90\n",
            "19 steps out of 49 steps: Average_loss: 16.46809 and Accuracy: 10.60\n",
            "20 steps out of 49 steps: Average_loss: 16.54177 and Accuracy: 10.20\n",
            "21 steps out of 49 steps: Average_loss: 16.94703 and Accuracy: 8.00\n",
            "22 steps out of 49 steps: Average_loss: 16.46809 and Accuracy: 10.60\n",
            "23 steps out of 49 steps: Average_loss: 16.85492 and Accuracy: 8.50\n",
            "24 steps out of 49 steps: Average_loss: 16.50493 and Accuracy: 10.40\n",
            "25 steps out of 49 steps: Average_loss: 16.61545 and Accuracy: 9.80\n",
            "26 steps out of 49 steps: Average_loss: 16.78124 and Accuracy: 8.90\n",
            "27 steps out of 49 steps: Average_loss: 16.56019 and Accuracy: 10.10\n",
            "28 steps out of 49 steps: Average_loss: 16.37599 and Accuracy: 11.10\n",
            "29 steps out of 49 steps: Average_loss: 16.52335 and Accuracy: 10.30\n",
            "30 steps out of 49 steps: Average_loss: 16.67072 and Accuracy: 9.50\n",
            "31 steps out of 49 steps: Average_loss: 16.41283 and Accuracy: 10.90\n",
            "32 steps out of 49 steps: Average_loss: 16.67072 and Accuracy: 9.50\n",
            "33 steps out of 49 steps: Average_loss: 17.11281 and Accuracy: 7.10\n",
            "34 steps out of 49 steps: Average_loss: 16.52335 and Accuracy: 10.30\n",
            "35 steps out of 49 steps: Average_loss: 16.50493 and Accuracy: 10.40\n",
            "36 steps out of 49 steps: Average_loss: 16.35756 and Accuracy: 11.20\n",
            "37 steps out of 49 steps: Average_loss: 16.56019 and Accuracy: 10.10\n",
            "38 steps out of 49 steps: Average_loss: 16.52335 and Accuracy: 10.30\n",
            "39 steps out of 49 steps: Average_loss: 16.68914 and Accuracy: 9.40\n",
            "40 steps out of 49 steps: Average_loss: 16.46809 and Accuracy: 10.60\n",
            "41 steps out of 49 steps: Average_loss: 16.70756 and Accuracy: 9.30\n",
            "42 steps out of 49 steps: Average_loss: 16.39441 and Accuracy: 11.00\n",
            "43 steps out of 49 steps: Average_loss: 16.57861 and Accuracy: 10.00\n",
            "44 steps out of 49 steps: Average_loss: 16.61545 and Accuracy: 9.80\n",
            "45 steps out of 49 steps: Average_loss: 16.48651 and Accuracy: 10.50\n",
            "46 steps out of 49 steps: Average_loss: 16.43125 and Accuracy: 10.80\n",
            "47 steps out of 49 steps: Average_loss: 16.92861 and Accuracy: 8.10\n",
            "48 steps out of 49 steps: Average_loss: 16.72598 and Accuracy: 9.20\n",
            "49 steps out of 49 steps: Average_loss: 16.50493 and Accuracy: 10.40\n",
            "Testing\n",
            "Test loss: 0.10138\n",
            "Test Accuracy: 101.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of convolutional Layer"
      ],
      "metadata": {
        "id": "6hqv5YS0Zok_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the activation function (e.g., ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
        "\n",
        "# Load and preprocess your dataset (replace this with your data loading code)\n",
        "# Make sure your data is loaded into train_images, train_labels, test_images, and test_labels\n",
        "\n",
        "# Data preprocessing (normalize, reshape, etc.)\n",
        "def preprocess_data(images):\n",
        "    images = images / 255.0  # Normalize pixel values to the range [0, 1]\n",
        "    images = images.reshape(images.shape[0], -1)  # Flatten images\n",
        "    return images\n",
        "\n",
        "train_images = preprocess_data(train_images)\n",
        "test_images = preprocess_data(test_images)\n",
        "\n",
        "# Define the neural network architecture\n",
        "input_size = train_images.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(0)\n",
        "weights_input_hidden = np.random.randn(hidden_size, input_size) * 0.01\n",
        "bias_hidden = np.zeros((hidden_size, 1))\n",
        "weights_hidden_output = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bias_output = np.zeros((output_size, 1))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_images), batch_size):\n",
        "        batch_images = train_images[i:i+batch_size]\n",
        "        batch_labels = train_labels[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        z1 = np.dot(weights_input_hidden, batch_images.T) + bias_hidden\n",
        "        a1 = relu(z1)\n",
        "        z2 = np.dot(weights_hidden_output, a1) + bias_output\n",
        "        a2 = softmax(z2)\n",
        "\n",
        "        # Compute loss (cross-entropy)\n",
        "        m = batch_labels.shape[0]\n",
        "        loss = -np.log(a2[batch_labels, range(m)]).mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        dz2 = a2\n",
        "        dz2[batch_labels, range(m)] -= 1\n",
        "        dw2 = np.dot(dz2, a1.T) / m\n",
        "        db2 = np.sum(dz2, axis=1, keepdims=True) / m\n",
        "        dz1 = np.dot(weights_hidden_output.T, dz2) * (a1 > 0)\n",
        "        dw1 = np.dot(dz1, batch_images) / m\n",
        "        db1 = np.sum(dz1, axis=1, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        weights_hidden_output -= learning_rate * dw2\n",
        "        bias_output -= learning_rate * db2\n",
        "        weights_input_hidden -= learning_rate * dw1\n",
        "        bias_hidden -= learning_rate * db1\n",
        "\n",
        "    # Print loss for this epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Testing\n",
        "test_z1 = np.dot(weights_input_hidden, test_images.T) + bias_hidden\n",
        "test_a1 = relu(test_z1)\n",
        "test_z2 = np.dot(weights_hidden_output, test_a1) + bias_output\n",
        "test_a2 = softmax(test_z2)\n",
        "\n",
        "predictions = np.argmax(test_a2, axis=0)\n",
        "accuracy = (predictions == test_labels).mean()\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")import numpy as np\n",
        "\n",
        "class Conv2dLayer:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # Initialize the weights and biases\n",
        "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
        "        self.biases = np.zeros((out_channels,))\n",
        "\n",
        "        # Store the input and output shapes for later use\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input_shape = x.shape\n",
        "        batch_size, in_channels, in_height, in_width = x.shape\n",
        "        out_height = int((in_height - self.kernel_size) / self.stride) + 1\n",
        "        out_width = int((in_width - self.kernel_size) / self.stride) + 1\n",
        "\n",
        "        self.output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
        "\n",
        "        # Initialize the output\n",
        "        output = np.zeros(self.output_shape)\n",
        "\n",
        "        # Perform the forward pass\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                h_start = i * self.stride\n",
        "                h_end = h_start + self.kernel_size\n",
        "                w_start = j * self.stride\n",
        "                w_end = w_start + self.kernel_size\n",
        "\n",
        "                x_slice = x[:, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                # Perform the convolution operation\n",
        "                output[:, :, i, j] = np.sum(x_slice * self.weights, axis=(1, 2, 3)) + self.biases\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        batch_size, out_channels, out_height, out_width = d_out.shape\n",
        "\n",
        "        # Initialize the gradients\n",
        "        d_weights = np.zeros(self.weights.shape)\n",
        "        d_biases = np.sum(d_out, axis=(0, 2, 3))\n",
        "        d_input = np.zeros(self.input_shape)\n",
        "\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                h_start = i * self.stride\n",
        "                h_end = h_start + self.kernel_size\n",
        "                w_start = j * self.stride\n",
        "                w_end = w_start + self.kernel_size\n",
        "\n",
        "                x_slice = x[:, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                for k in range(out_channels):\n",
        "                    d_weights[k, :, :, :] += np.sum(x_slice * d_out[:, k, i, j][:, np.newaxis, np.newaxis, np.newaxis], axis=0)\n",
        "                    d_input[:, :, h_start:h_end, w_start:w_end] += self.weights[k, :, :, :] * d_out[:, k, i, j][:, np.newaxis, np.newaxis, np.newaxis]\n",
        "\n",
        "        return d_input, d_weights, d_biases\n",
        "\n",
        "# Create an example input\n",
        "x = np.random.randn(2, 3, 6, 6)\n",
        "\n",
        "# Create a convolutional layer\n",
        "conv_layer = Conv2dLayer(in_channels=3, out_channels=2, kernel_size=3, stride=2)\n",
        "\n",
        "# Forward pass\n",
        "output = conv_layer.forward(x)\n",
        "print(\"Forward Pass Output:\")\n",
        "print(output)\n",
        "\n",
        "# Compute a random gradient for demonstration\n",
        "d_out = np.random.randn(2, 2, 2, 2)\n",
        "\n",
        "# Backward pass\n",
        "d_input, d_weights, d_biases = conv_layer.backward(d_out)\n",
        "print(\"\\nBackward Pass - Gradient w.r.t. Input:\")\n",
        "print(d_input)\n",
        "print(\"\\nBackward Pass - Gradient w.r.t. Weights:\")\n",
        "print(d_weights)\n",
        "print(\"\\nBackward Pass - Gradient w.r.t. Biases:\")\n",
        "print(d_biases)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9H0Z920F5hJ",
        "outputId": "a28c1512-1078-465c-df07-6742d89de353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3016\n",
            "Epoch 2, Loss: 2.3015\n",
            "Epoch 3, Loss: 2.3016\n",
            "Epoch 4, Loss: 2.3016\n",
            "Epoch 5, Loss: 2.3016\n",
            "Epoch 6, Loss: 2.3016\n",
            "Epoch 7, Loss: 2.3016\n",
            "Epoch 8, Loss: 2.3016\n",
            "Epoch 9, Loss: 2.3016\n",
            "Epoch 10, Loss: 2.3015\n",
            "Test Accuracy: 9.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jZAo6WkOo84"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}